# Hadoop与Spark

## 1、hadoop

1）hadoop简介

​       Hadoop是一个由Apache基金会所开发的分布式系统基础架构。  Hadoop实现了一个分布式文件系统HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的硬件上；而且它提供高吞吐量来访问应用程序的数据，适合那些有着超大数据集的应用程序。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算

2）hadoop优点

Hadoop 以一种可靠、高效、可伸缩的方式进行数据处理。

**可靠性**: Hadoop将数据存储在多个备份，Hadoop提供高吞吐量来访问应用程序的数据。

**高扩展性**： Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。

**高效性**： Hadoop以并行的方式工作，通过并行处理加快处理速度。

**高容错性**： Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。

**低成本**： Hadoop能够部署在低廉的（low-cost）硬件上。

## **2、spark**

1）spark简介

​      Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark拥有Hadoop  MapReduce所具有的优点，Spark在Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark性能以及运算速度高于MapReduce。



2）spark优点

**计算速度快**: 因为spark从磁盘中读取数据，把中间数据放到内存中，，完成所有必须的分析处理，将结果写回集群，所以spark更快。

**Spark 提供了大量的库**: 包括Spark Core、Spark SQL、Spark Streaming、MLlib、GraphX。

**支持多种资源管理器**: Spark 支持 Hadoop YARN，及其自带的独立集群管理器

**操作简单**: 高级 API 剥离了对集群本身的关注，Spark 应用开发者可以专注于应用所要做的计算本身



## 3、spark与hadoop的不同点

1）应用场景不同

Hadoop和Spark两者都是大数据框架，但是各自应用场景是不同的。Hadoop是一个分布式数据存储架构，它将巨大的数据集分派到一个由普通计算机组成的集群中的多个节点进行存储，降低了硬件的成本。Spark是那么一个专门用来对那些分布式存储的大数据进行处理的工具，它要借助hdfs的数据存储。

2）处理速度不同

hadoop的MapReduce是分步对数据进行处理的，从磁盘中读取数据，进行一次处理，将结果写到磁盘，然后在从磁盘中读取更新后的数据，再次进行的处理，最后再将结果存入磁盘，这存取磁盘的过程会影响处理速度。spark从磁盘中读取数据，把中间数据放到内存中，，完成所有必须的分析处理，将结果写回集群，所以spark更快。

3）容错性不同

Hadoop将每次处理后的数据都写入到磁盘上，基本谈不上断电或者出错数据丢失的情况。Spark的数据对象存储在弹性分布式数据集 RDD，RDD是分布在一组节点中的只读对象集合，如果数据集一部分丢失，则可以根据于数据衍生过程对它们进行重建。而且RDD 计算时可以通过  CheckPoint 来实现容错。

## 4、spark与hadoop的联系

Hadoop提供分布式数据存储功能HDFS，还提供了用于数据处理的MapReduce。  MapReduce是可以不依靠spark数据的处理的。当然spark也可以不依靠HDFS进行运作，它可以依靠其它的分布式文件系统。但是两者完全可以结合在一起，hadoop提供分布式 集群和分布式 文件系统，spark可以依附在hadoop的HDFS代替MapReduce弥补MapReduce计算能力不足的问题。

**总结一句话：spark在hadoop肩膀上可以让大数据跑的更快**